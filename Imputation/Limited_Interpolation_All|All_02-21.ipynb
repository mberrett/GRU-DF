{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM_RNN\n",
    "### 6M|ALL\n",
    "#### 4D|ALL\n",
    "- Imputation: \n",
    "    - No Imputaition for the target EDSS\n",
    "    - Interpolation for the rest of features\n",
    "- Features: All features\n",
    "- Time Steps: 34\n",
    "- Evaluation: MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiasberretta/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(col_list, n_months):\n",
    "    \n",
    "    \"\"\"takes in a list of column names and number of visits starting at 0\n",
    "    returns column list time-stepped and dovetailed\"\"\" \n",
    "    \n",
    "    return dovetail_names(*[time_step_names(i, n_months) for i in col_list])\n",
    "        \n",
    "def time_step_names(name, n_months):\n",
    "\n",
    "    return [(name + '_%d' % (j+1)) for j in range(-1,n_months*6, 6)]\n",
    "\n",
    "def dovetail_names(*kwargs):\n",
    "    zipped = zip(*kwargs)\n",
    "    l = []\n",
    "    for i in zipped:\n",
    "        for j in i:\n",
    "            l.append(j)\n",
    "    return l\n",
    "\n",
    "def stretch_input(Xtr, n_inputs, time_steps, pot):\n",
    "\n",
    "    \"\"\"Xtr_fill is empty 3D numpy array where we extend length of patient observation times t\n",
    "    pot stands for Patient Observation Time. We only need to do this for our X input\"\"\"\n",
    "    \n",
    "    Xtr_fill = np.zeros(shape=[Xtr.shape[0],time_steps,n_inputs*pot] , dtype = object) \n",
    "\n",
    "    for subject in range(Xtr.shape[0]):\n",
    "    \n",
    "        for i in range(time_steps):\n",
    "\n",
    "            temp = np.concatenate([Xtr[subject][i],Xtr[subject][i+1],Xtr[subject][i+2],Xtr[subject][i+3]]) # changed for pot = 3\n",
    "            Xtr_fill[subject][i] = temp\n",
    "            \n",
    "    return Xtr_fill\n",
    "\n",
    "def stack_times(data, name, n):\n",
    "    \n",
    "    \n",
    "    \"\"\"takes in dataframe, column name and n of time steps\n",
    "    and puts it in long format\"\"\"\n",
    "    \n",
    "    all_names = select_columns(name, n-1)\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for col in all_names:\n",
    "        l.append(data[col].copy())\n",
    "    \n",
    "    stacked = l[0]\n",
    "    rest = l[1:]\n",
    "    \n",
    "    # stack Series and get dummy variables \n",
    "    stacked.append(rest)\n",
    "    \n",
    "    return stacked\n",
    "\n",
    "def stack_dummy(data, name, n):\n",
    "    \n",
    "    \n",
    "    \"\"\"takes in dataframe and column name\n",
    "    return that same feature split into dummy columns\n",
    "    across n time steps (adjacent)\"\"\"\n",
    "    \n",
    "    all_names = select_columns(name, n-1)\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for col in all_names:\n",
    "        l.append(data[col].copy())\n",
    "    \n",
    "    f = l[0]\n",
    "    rest = l[1:]\n",
    "    \n",
    "    # stack Series and get dummy variables \n",
    "    pre_dummy = pd.get_dummies(f.append(rest))\n",
    "    \n",
    "    after_dummy = time_dummy(pre_dummy, n)\n",
    "    \n",
    "    dummy_value_names = generate_col_names(after_dummy, name)\n",
    "    time_stepped_dummy_names = time_step_dummy_value_names(dummy_value_names, n)\n",
    "    \n",
    "    for t in range(len(after_dummy)):\n",
    "        \n",
    "        after_dummy[t].columns = list(time_stepped_dummy_names[t])\n",
    "        \n",
    "    #untimed_names_to_order = column_names_per_time_step(col_names_together, \"what are you\", name[0])\n",
    "    #names_to_order = select_columns(untimed_names_to_order, n-1)\n",
    "\n",
    "    return pd.concat(after_dummy, axis = 1, sort = False), dummy_value_names\n",
    "\n",
    "\n",
    "def time_dummy(dummy_df, n):\n",
    "    \n",
    "    \"\"\"Separates long data frame into time steps \n",
    "    (508 subjects (rows) per time step)\"\"\"\n",
    "    \n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        l.append(dummy_df.iloc[i*508:(i+1)*508,:].copy())\n",
    "    \n",
    "    return l\n",
    "\n",
    "def generate_col_names(after_dummy, name):\n",
    "    \n",
    "    \"\"\"Generates column names for result of pd.get_dummies on a feature\n",
    "    i.e. if A has values x and y, it will generate A_x, A_y\"\"\"\n",
    "    \n",
    "    return [(str(name[0]) + \"_\" + str(list(after_dummy[0].columns)[i])) for i in range(len(list(after_dummy[0].columns)))]\n",
    "\n",
    "def time_step_dummy_value_names(names, n_months):\n",
    "    \n",
    "    long_list = [(name + '_%d' % (j+1)) for j in range(-1,n_months*6, 6) for name in names]\n",
    "    return np.array(long_list).reshape(-1, len(names))\n",
    "\n",
    "def add_columns(add_to, name, names_per_t, n):\n",
    "    \n",
    "    n = n + 1\n",
    "    to_add, bare_names = stack_dummy(df, name, n)\n",
    "    to_remove = select_columns(name, n-1)\n",
    "    \n",
    "    \"\"\"add new dummied features to dataframes (copy)\n",
    "    and remove undummied version of features\n",
    "    name is a list\n",
    "    \n",
    "    encompasses stack_dummy\"\"\"\n",
    "    \n",
    "    newdf = add_to.copy()\n",
    "    column_names = list(to_add.columns)\n",
    "    \n",
    "    for i in range(len(column_names)):\n",
    "        newdf[column_names[i]] = to_add.iloc[:,i]\n",
    "    newdf.drop(to_remove,axis = 1, inplace = True)\n",
    "    \n",
    "    #print(bare_names, name[0])\n",
    "    \n",
    "    names_per_t_updated = column_names_per_time_step(names_per_t, bare_names, name[0])\n",
    "    namesOrder= select_columns(names_per_t_updated, n-1)\n",
    "    return newdf[namesOrder].copy(), names_per_t_updated\n",
    "\n",
    "#    return names_per_t_updated\n",
    "#     print(name[0])\n",
    "    \n",
    "    \n",
    "    return newdf[namesOrder].copy(), names_per_t_updated\n",
    "\n",
    "\n",
    "def column_names_per_time_step(original_list, add, remove):\n",
    "    \"\"\"makes sure EDSS stays at the end\n",
    "    remove pre \"\"\"\n",
    "    \n",
    "    new_list = original_list.copy()\n",
    "    \n",
    "    \n",
    "    new_list.remove(remove)\n",
    "    new_list.extend(add)\n",
    "    \n",
    "    # makes sure EDSS is always last\n",
    "    \n",
    "    new_list.remove('EDSS')\n",
    "    new_list.append('EDSS')\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "def manual_dummy(df, names, name_list, n):\n",
    "    \n",
    "    dfUpdated =df.copy()\n",
    "    names = [[name] for name in names] # turn to list foramt so that it works\n",
    "    \n",
    "    for name in names:\n",
    "        \n",
    "        dfUpdated, name_list = add_columns(dfUpdated, name , name_list, n)\n",
    "   \n",
    "    return dfUpdated # should I return name_list as well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA & DEFINE N OF TIME STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"newdata_11.22.csv\", index_col = 0)\n",
    "dfForColumnNames = pd.read_csv(\"zeroImputed_dfByVisitDate_21-11.csv\", index_col = 0)\n",
    "\n",
    "\"\"\"DEFINE N\"\"\"\n",
    "n = 37 \n",
    "# n determines the amount of time steps in the data after 0, (n = 1) == (2 data points), (n = 3) == (4 data points)\n",
    "# i.e. n = 2 means we have three time steps t0, t1, t2, which together encompass one year of patient observation\n",
    "# n = 5 means we have 6 time steps (t0...t5), which encompass 2 year and half of patient observation \n",
    "\n",
    "\"\"\"DEFINE POT\"\"\"\n",
    "pot = 1 # patient observation length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed feature columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 23 fixed features\n"
     ]
    }
   ],
   "source": [
    "fixed_feature_column_names = list(dataset.iloc[:, 1:24].columns)\n",
    "\n",
    "print(\"We have\", len(fixed_feature_column_names), \"fixed features\")\n",
    "\n",
    "timed_ff_names = select_columns(fixed_feature_column_names,n-1) # for x variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time sensitive columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 45 timed features\n"
     ]
    }
   ],
   "source": [
    "timed_feature_column_names = list(dfForColumnNames.iloc[:,2:].columns)\n",
    "timed_feature_column_names.remove('EDSS')\n",
    "timed_feature_column_names.extend(['EDSS']) # position EDSS at the end always for tensorflow loss function \n",
    "\n",
    "print(\"We have\", len(timed_feature_column_names), \"timed features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join fixed features and time sensitive columns lists\n",
    "\n",
    "Join fixed features and time sensitive columns and generate the appropriate column list \n",
    "where each time step contains both fixed features and timed features\n",
    "the caveat being that fixed features retain a constant value across time steps\n",
    "whereas time sensitive features do not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 total amount of features before one-hot-encoding spread across 37 timesteps generate 2584 columns\n"
     ]
    }
   ],
   "source": [
    "col_names_together = fixed_feature_column_names.copy()\n",
    "col_names_together.extend(timed_feature_column_names)\n",
    "\n",
    "col_names_together_timed = select_columns(col_names_together, n)\n",
    "\n",
    "print(len(col_names_together), \"total amount of features before one-hot-encoding\",\n",
    "     \"spread across\", n, \"timesteps generate\", len(col_names_together_timed), \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add fixed feature values to the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.08695652173913 %\n",
      "['SIBLINGS', 'BIRTH_ORDER', 'EDUCATION_DEGREE', 'EDUCATION_DEGREE_DESC', 'CHILD', 'DOMINANT_HAND']\n"
     ]
    }
   ],
   "source": [
    "df_fixed = dataset[fixed_feature_column_names].copy() \n",
    "df_fixed_columns_with_nans = df_fixed.columns[df_fixed.isna().any()].tolist()\n",
    "df_fixed_columns_with_nans\n",
    "print(len(df_fixed_columns_with_nans)/len(df_fixed.columns)*100, \"%\")\n",
    "print(df_fixed_columns_with_nans)\n",
    "\n",
    "# (mode)impute for missing fixed values by column mode\n",
    "df_fixed.fillna(df_fixed.mode().iloc[0], axis = 0, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dataset = dataset.copy()\n",
    "\n",
    "# assign the same value at each time step for the fixed feature columns\n",
    "# which are already appropriately interleaved by time in the X space \n",
    "for initial_name in fixed_feature_column_names:\n",
    "    for col_name in select_columns([initial_name], n):\n",
    "        m_dataset[col_name] = df_fixed[initial_name]# .fillna(0, inplace=True) # replace all missing values as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in EDSS simple - Needs Review\n",
    "\n",
    "will require interpolation within training loop later one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data set to timed columns, both fixed and time sensitive \n",
    "df = m_dataset[col_names_together_timed].copy()\n",
    "\n",
    "# Y to 1 replacement: Categorical\n",
    "y_replacement_categorical = [\"IMAGES\", \"BRAIN\", \"BRAIN_T2\",\"BRAIN_GAD\",\"BRAIN_UNCHANGED\",\"SPINE\",\"TSPINE\",\"CSPINE\",\"LSPINE\",\"USPINE\",\"SPINE_T2\",\"SPINE_GAD\",\"SPINE_UNCHANGED\"]\n",
    "\n",
    "for column in y_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('Y', 1, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# N to 0 replacement: Categorical\n",
    "n_replacement_categorical = [\"AMBULATORY_INDEX\"]\n",
    "\n",
    "for column in n_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('N', -1, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "        \n",
    "# Y to 1, N to 0 replacement: Categorical \n",
    "yn_replacement_categorical = [\"AMPYRA\", \"NORMAL_BRAIN\", \"NORMAL_SPINE\",\"SMOKING_HISTORY\"]\n",
    "\n",
    "for column in yn_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('Y', 1, inplace=True)\n",
    "        df[col].replace('N', -1, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# R to 1, L to -1 replacement: Categorical \n",
    "rl_replacement_categorical = [\"DOMINANT_HAND\"]\n",
    "\n",
    "for column in rl_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('R', 1, inplace=True)\n",
    "        df[col].replace('L', -1, inplace=True)\n",
    "        df[col].replace('A', 0, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# R to 1, L to -1 replacement: Categorical \n",
    "mf_replacement_categorical = [\"SEX\"]\n",
    "\n",
    "for column in mf_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('M', 1, inplace=True) # like Tom\n",
    "        df[col].replace('F', -1, inplace=True) # like Tom\n",
    "        df[col].replace('O', 0, inplace=True) # like Tom\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# X to 0 replacement categorical\n",
    "x_replacement_categorical = [\"CEREBELLAR_FUNCTION\",\"BRAINSTEM_FUNCTION\",\"SENSORY_FUNCTION\",\"BOWEL_BLADDER_FUNCTION\",\"VISUAL_FUNCTION\", \"FEET25_ASSISTANCE\"]\n",
    "\n",
    "for column in x_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('X', 0, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# X to 0 replacement categorical\n",
    "p_replacement_categorical = [\"PROTOCOL\"]\n",
    "\n",
    "for column in p_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('3T', 0, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "\n",
    "# X to 0 replacement categorical\n",
    "g_replacement_categorical = [\"T3_BPF\"]\n",
    "\n",
    "for column in g_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col].replace('G', 0, inplace=True)\n",
    "        df[col].replace('B', 0, inplace=True)\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "        \n",
    "\n",
    "# X to 0 replacement categorical\n",
    "dob_replacement_categorical = [\"DOB_YEAR\"]\n",
    "\n",
    "for column in dob_replacement_categorical:\n",
    "    for col in select_columns([column], n):\n",
    "        df[col] = (df[col]-1900)/10\n",
    "        #df[col].fillna(df.mode().iloc[0], inplace = True)\n",
    "        \n",
    "#df.replace('O', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all columns ending with \"_desc\"\n",
    "\n",
    "- \"RACE_DESC\" == \"RACE\"\n",
    "- \"ETHNICITY_DESC\" == \"ETHNICITY\"\n",
    "- \"EDUCATION_DEGREE_DESC\" == \"EDUCATION_DEGREE\"\n",
    "- \"MARITAL_DESC\" ==  \"MARITAL\"\n",
    "- \"TWIN_DESC\" == \"TWIN\"\n",
    "- \"AUTOIMM_DESC\" \n",
    "- \"FAMILY_MS_DESC\" \n",
    "- \"FAMILY_AI_DESC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"RACE_DESC\", \"ETHNICITY_DESC\", \"EDUCATION_DEGREE_DESC\", \n",
    "                   \"MARITAL_DESC\", \"TWIN_DESC\", \"AUTOIMM_DESC\", \n",
    "                   \"FAMILY_MS_DESC\", \"FAMILY_AI_DESC\"]\n",
    "dfPreDummy = df.drop(columns = select_columns(columns_to_drop, n)).copy()\n",
    "\n",
    "fillin_df = dfPreDummy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to dummy\n",
    "\n",
    "- \"MARITAL\"\n",
    "- \"AUTOMIMMUN\"\n",
    "- \"FAMILY_MS\" \n",
    "- \"FAMILY_AI\" \n",
    "- \"ATTACK\" \n",
    "- \"STATUS\" \n",
    "- \"PROTOCOL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_dummy = [\"RACE\",\"MARITAL\",\"AUTOMIMMUN\",\"FAMILY_MS\", \"FAMILY_AI\", \"ATTACK\", \"STATUS\"]\n",
    "initial_column_list = [str(i)[:-2] for i in dfPreDummy.columns[:60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = [str(x)[:-2] for x in dfPreDummy.columns[:110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_interpolate = [x for x in all_cols if x not in \n",
    "                          [x for x in df_fixed.columns.tolist() if x not in columns_to_drop]]\n",
    "\n",
    "columns_to_interpolate.remove('EDSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mode_across_time(df, col):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    to_impute = df_copy[select_columns([col], n)].copy()\n",
    "    to_impute = to_impute.T.fillna(to_impute.mode(axis=1)[0]).T\n",
    "    \n",
    "    for col in to_impute.columns:\n",
    "        df_copy[col] = to_impute[col] \n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "m1 = impute_mode_across_time(fillin_df, 'ATTACK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matiasberretta/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py:838: UserWarning: Unable to sort modes: '<' not supported between instances of 'str' and 'int'\n",
      "  warn(\"Unable to sort modes: {error}\".format(error=e))\n",
      "/Users/matiasberretta/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py:838: UserWarning: Unable to sort modes: '<' not supported between instances of 'int' and 'str'\n",
      "  warn(\"Unable to sort modes: {error}\".format(error=e))\n",
      "/Users/matiasberretta/anaconda3/lib/python3.6/site-packages/pandas/core/algorithms.py:838: UserWarning: Unable to sort modes: '<' not supported between instances of 'str' and 'float'\n",
      "  warn(\"Unable to sort modes: {error}\".format(error=e))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EDSS_0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = impute_mode_across_time(fillin_df, 'ATTACK')\n",
    "\n",
    "for i in range(1, len(columns_to_interpolate)):\n",
    "    \n",
    "    final = impute_mode_across_time(final, columns_to_interpolate[i])\n",
    "    \n",
    "final.columns[final.isna().any()].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 EDSS_0\n",
      "110.0\n",
      "Before dummification, there are 60 features per time step\n"
     ]
    }
   ],
   "source": [
    "updatedDf = manual_dummy(final, columns_to_dummy, initial_column_list, n)\n",
    "\n",
    "n_inputs_pure = list(updatedDf.columns).index('EDSS_0')+1\n",
    "print(n_inputs_pure, list(updatedDf.columns)[n_inputs_pure-1])\n",
    "untimed_input_names = [str(x)[:-2] for x in updatedDf.columns[:110]] #np.flaot \n",
    "print(updatedDf.iloc[:,:-110].shape[1]/n)\n",
    "\n",
    "print(\"Before dummification, there are\" , len(list(dfPreDummy.columns)[:60]), \"features per time step\")\n",
    "\n",
    "#fillin_df = updatedDf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data in Different Time Arrangements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6m|all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_6m(sc):\n",
    "    \n",
    "    sc2 = sc + 1\n",
    "    sc3 = sc\n",
    "    \n",
    "    nanY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY = updatedDf[select_columns(['EDSS'], 0)].copy() # only interpolate for t0\n",
    "    interpolateY.interpolate(axis = 1, limit_direction = \"both\", inplace = True)\n",
    "    interpolateX = interpolateY.drop(list(interpolateY.columns)[-sc2:],axis = 1).copy()\n",
    "\n",
    "    X = updatedDf.iloc[:,:(n_inputs_pure*(n-sc))].copy()\n",
    "\n",
    "    for col in interpolateX.columns:\n",
    "        X[col] = interpolateX[col] \n",
    "\n",
    "    for col in interpolateY.columns:\n",
    "        nanY[col] = interpolateY[col] \n",
    "\n",
    "    y = nanY.drop(select_columns(['EDSS'], sc3), axis = 1).copy() \n",
    "\n",
    "    X.replace('O', 0, inplace = True)\n",
    "    \n",
    "    mask = X.index.isin(X[X[\"EDSS_0\"].notnull()].index.tolist())\n",
    "    \n",
    "    X, y = X[mask], y[mask]\n",
    "    print(X.shape, y.shape)\n",
    "    print(\"First time sliceof X ends at \", X.columns.tolist()[109])\n",
    "    print(\"First y column is\", y.columns.tolist()[0])\n",
    "\n",
    "    print(\"Last column for X is \",X.columns.tolist()[-1])\n",
    "    print(\"Last y column is \", y.columns.tolist()[-1])\n",
    "    \n",
    "    X.reset_index(inplace=True, drop = True)\n",
    "    y.reset_index(inplace=True, drop = True)\n",
    "    \n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491, 4070) (491, 37)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_6\n",
      "Last column for X is  EDSS_216\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_6m(0)\n",
    "\n",
    "X.to_csv(\"../limit_interp_data/X_6_months|6_months_limited_interpolation.csv\")\n",
    "y.to_csv(\"../limit_interp_data/y_6_months|6_months_limited_interpolation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 4070) (508, 37)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_6\n",
      "Last column for X is  EDSS_216\n",
      "Last y column is  EDSS_222\n",
      "(508, 3960) (508, 36)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_12\n",
      "Last column for X is  EDSS_210\n",
      "Last y column is  EDSS_222\n",
      "(508, 3850) (508, 35)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_18\n",
      "Last column for X is  EDSS_204\n",
      "Last y column is  EDSS_222\n",
      "(508, 3740) (508, 34)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_24\n",
      "Last column for X is  EDSS_198\n",
      "Last y column is  EDSS_222\n",
      "(508, 3630) (508, 33)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_30\n",
      "Last column for X is  EDSS_192\n",
      "Last y column is  EDSS_222\n",
      "(508, 3520) (508, 32)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_36\n",
      "Last column for X is  EDSS_186\n",
      "Last y column is  EDSS_222\n",
      "(508, 3410) (508, 31)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_42\n",
      "Last column for X is  EDSS_180\n",
      "Last y column is  EDSS_222\n",
      "(508, 3300) (508, 30)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_48\n",
      "Last column for X is  EDSS_174\n",
      "Last y column is  EDSS_222\n",
      "(508, 3190) (508, 29)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_54\n",
      "Last column for X is  EDSS_168\n",
      "Last y column is  EDSS_222\n",
      "(508, 3080) (508, 28)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_60\n",
      "Last column for X is  EDSS_162\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_6m(1)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|1_year_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|1_year_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(2)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|1.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|1.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(3)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|2_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|2_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(4)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|2.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|2.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(5)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|3_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|3_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(6)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|3.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|3.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(7)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|4_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|4_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(8)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|4.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|4.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_6m(9)\n",
    "\n",
    "X.to_csv(\"../data/X_6_months|5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_6_months|5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 year | All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1y(sc):\n",
    "    \n",
    "    sc2 = sc - 1\n",
    "    sc3 = sc \n",
    "\n",
    "    nanY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY.interpolate(axis = 1, limit_direction = \"both\", inplace = True)\n",
    "    interpolateX = interpolateY.drop(list(interpolateY.columns)[-sc:],axis = 1).copy()\n",
    "\n",
    "    X = updatedDf.iloc[:,:(n_inputs_pure*(n-sc2))].copy()\n",
    "    y = nanY.drop(select_columns(['EDSS'], sc3), axis = 1).copy()\n",
    "    print(X.shape, y.shape) \n",
    "\n",
    "    for col in interpolateX.columns:\n",
    "        X[col] = interpolateX[col] \n",
    "\n",
    "    X.replace('O', 0, inplace = True)\n",
    "    print(\"First time sliceof X ends at \", X.columns.tolist()[109])\n",
    "    print(\"First y column is\", y.columns.tolist()[0])\n",
    "\n",
    "    print(\"Last column for X is \",X.columns.tolist()[-1])\n",
    "    print(\"Last y column is \", y.columns.tolist()[-1])\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 4070) (508, 36)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_12\n",
      "Last column for X is  EDSS_216\n",
      "Last y column is  EDSS_222\n",
      "(508, 3960) (508, 35)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_18\n",
      "Last column for X is  EDSS_210\n",
      "Last y column is  EDSS_222\n",
      "(508, 3850) (508, 34)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_24\n",
      "Last column for X is  EDSS_204\n",
      "Last y column is  EDSS_222\n",
      "(508, 3740) (508, 33)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_30\n",
      "Last column for X is  EDSS_198\n",
      "Last y column is  EDSS_222\n",
      "(508, 3630) (508, 32)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_36\n",
      "Last column for X is  EDSS_192\n",
      "Last y column is  EDSS_222\n",
      "(508, 3520) (508, 31)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_42\n",
      "Last column for X is  EDSS_186\n",
      "Last y column is  EDSS_222\n",
      "(508, 3410) (508, 30)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_48\n",
      "Last column for X is  EDSS_180\n",
      "Last y column is  EDSS_222\n",
      "(508, 3300) (508, 29)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_54\n",
      "Last column for X is  EDSS_174\n",
      "Last y column is  EDSS_222\n",
      "(508, 3190) (508, 28)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_60\n",
      "Last column for X is  EDSS_168\n",
      "Last y column is  EDSS_222\n",
      "(508, 3080) (508, 27)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_66\n",
      "Last column for X is  EDSS_162\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_1y(1)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|6_months_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|6_months_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(2)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|1_year_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|1_year_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(3)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|1.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|1.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(4)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|2_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|2_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(5)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|2.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|2.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(6)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|3_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|3_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(7)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|3.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|3.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(8)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|4_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|4_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y(9)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|4.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|4.5_years_exhaustive.csv\")\n",
    "\n",
    "\n",
    "X, y = generate_data_1y(10)\n",
    "\n",
    "X.to_csv(\"../data/X_1_year|5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1_year|5_years_exhaustive.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 years|All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1y6m(sc):\n",
    "    \n",
    "    sc2 = sc - 1\n",
    "    sc3 = sc + 1\n",
    "\n",
    "    nanY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY.interpolate(axis = 1, limit_direction = \"both\", inplace = True)\n",
    "    interpolateX = interpolateY.drop(list(interpolateY.columns)[-sc:],axis = 1).copy()\n",
    "\n",
    "    X = updatedDf.iloc[:,:(n_inputs_pure*(n-sc2))].copy()\n",
    "    y = nanY.drop(select_columns(['EDSS'], sc3), axis = 1).copy()\n",
    "    print(X.shape, y.shape) \n",
    "\n",
    "    for col in interpolateX.columns:\n",
    "        X[col] = interpolateX[col] \n",
    "\n",
    "    X.replace('O', 0, inplace = True)\n",
    "    print(\"First time sliceof X ends at \", X.columns.tolist()[109])\n",
    "    print(\"First y column is\", y.columns.tolist()[0])\n",
    "\n",
    "    print(\"Last column for X is \",X.columns.tolist()[-1])\n",
    "    print(\"Last y column is \", y.columns.tolist()[-1])\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 4070) (508, 35)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_18\n",
      "Last column for X is  EDSS_216\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_1y6m(1)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|6_months_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|6_months_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(2)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|1_year_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|1_year_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(3)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|1.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|1.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(4)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|2_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|2_years_exhaustive.csv\")\n",
    "\n",
    "\n",
    "X, y = generate_data_1y6m(5)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|2.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|2.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(6)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|3_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|3_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(7)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|3.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|3.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(8)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|4_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|4_years_exhaustive.csv\")\n",
    "\n",
    "\n",
    "X, y = generate_data_1y6m(9)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|4.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|4.5_years_exhaustive.csv\")\n",
    "\n",
    "X, y = generate_data_1y6m(10)\n",
    "\n",
    "X.to_csv(\"../data/X_1.5_years|5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_1.5_years|5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2y|All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_2y(sc):\n",
    "    \n",
    "    sc2 = sc - 1\n",
    "    sc3 = sc + 2\n",
    "\n",
    "    nanY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY = updatedDf[select_columns(['EDSS'], n)].copy()\n",
    "    interpolateY.interpolate(axis = 1, limit_direction = \"both\", inplace = True)\n",
    "    interpolateX = interpolateY.drop(list(interpolateY.columns)[-sc:],axis = 1).copy()\n",
    "\n",
    "    X = updatedDf.iloc[:,:(n_inputs_pure*(n-sc2))].copy()\n",
    "    y = nanY.drop(select_columns(['EDSS'], sc3), axis = 1).copy()\n",
    "    print(X.shape, y.shape) \n",
    "\n",
    "    for col in interpolateX.columns:\n",
    "        X[col] = interpolateX[col] \n",
    "\n",
    "    X.replace('O', 0, inplace = True)\n",
    "    print(\"First time sliceof X ends at \", X.columns.tolist()[109])\n",
    "    print(\"First y column is\", y.columns.tolist()[0])\n",
    "\n",
    "    print(\"Last column for X is \",X.columns.tolist()[-1])\n",
    "    print(\"Last y column is \", y.columns.tolist()[-1])\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 4070) (508, 34)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_24\n",
      "Last column for X is  EDSS_216\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(1)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|6_months_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|6_months_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3960) (508, 33)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_30\n",
      "Last column for X is  EDSS_210\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(2)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|1_year_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|1_year_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3850) (508, 32)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_36\n",
      "Last column for X is  EDSS_204\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(3)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|1.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|1.5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3740) (508, 31)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_42\n",
      "Last column for X is  EDSS_198\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(4)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|2_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|2_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3630) (508, 30)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_48\n",
      "Last column for X is  EDSS_192\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(5)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|2.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|2.5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3520) (508, 29)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_54\n",
      "Last column for X is  EDSS_186\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(6)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|3_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|3_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3410) (508, 28)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_60\n",
      "Last column for X is  EDSS_180\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(7)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|3.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|3.5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3300) (508, 27)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_66\n",
      "Last column for X is  EDSS_174\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(8)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|4_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|4_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3190) (508, 26)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_72\n",
      "Last column for X is  EDSS_168\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(9)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|4.5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|4.5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(508, 3080) (508, 25)\n",
      "First time sliceof X ends at  EDSS_0\n",
      "First y column is EDSS_78\n",
      "Last column for X is  EDSS_162\n",
      "Last y column is  EDSS_222\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data_2y(10)\n",
    "\n",
    "X.to_csv(\"../data/X_2_years|5_years_exhaustive.csv\")\n",
    "y.to_csv(\"../data/y_2_years|5_years_exhaustive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
