{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Fill â€” 1 year and a half observation window\n",
    "### Prediction Imputation\n",
    "\n",
    "- For this model we do not mask the training set's missing instances. We impute them.\n",
    "- We do, however, perform masking for the teting data evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates\n",
    "- Remove all test operations\n",
    "- mask_list.append((train, test)) went to mask_list.append(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from keras import callbacks\n",
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import os, shutil\n",
    "\n",
    "from keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     10,
     14,
     36,
     55,
     78,
     89,
     107,
     119
    ]
   },
   "outputs": [],
   "source": [
    "pot = 3\n",
    "n_features = 115\n",
    "\n",
    "def select_columns(col_list, n_months):\n",
    "    \"\"\"\n",
    "    Takes in a list of column names and number of visits starting at 0.\n",
    "    Returns column list time-stepped and dovetailed.\n",
    "    \"\"\" \n",
    "    return dovetail_names(*[time_step_names(i, n_months) for i in col_list])\n",
    "        \n",
    "def time_step_names(name, n_months):\n",
    "\n",
    "    return [(name + '_%d' % (j+1)) for j in range(-1,n_months*6, 6)]\n",
    "\n",
    "def stretch_input(Xtr, time_steps, pot, n_features=n_features):\n",
    "    \"\"\"\n",
    "    Xtr_fill is empty 3D numpy array where we extend length of patient observation times t\n",
    "    pot stands for Patient Observation Time. We only need to do this for our X input\n",
    "    \"\"\"\n",
    "    Xtr_fill = np.zeros(shape=[Xtr.shape[0],time_steps,n_features*pot] , dtype = object) \n",
    "\n",
    "    for subject in range(Xtr.shape[0]):\n",
    "    \n",
    "        for i in range(time_steps):\n",
    "            \n",
    "            concat_list = []\n",
    "            \n",
    "            for extra in range(pot):\n",
    "                \n",
    "                concat_list.append(Xtr[subject][i+extra])\n",
    "                \n",
    "            temp = np.concatenate(concat_list) \n",
    "            Xtr_fill[subject][i] = temp\n",
    "            \n",
    "    return Xtr_fill\n",
    "\n",
    "def reshape_data(X, y, n_time_steps, pot=pot, n_features = n_features):  \n",
    "    \n",
    "    extra_ts = pot - 1\n",
    "    \n",
    "    X_reshaped = X.values.reshape(-1, n_time_steps+extra_ts, n_features)\n",
    "    y_reshaped = y.values.reshape(-1, n_time_steps, 1)\n",
    "    \n",
    "    if (pot > 1):\n",
    "        \n",
    "        X = stretch_input(X_reshaped, n_time_steps, pot)\n",
    "\n",
    "    y = y_reshaped.astype(float)\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    print(\"X reshaped is \" + str(X.shape))\n",
    "    print(\"y reshaped is \" + str(y_reshaped.shape))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def provide_data(X, y, roll, n_features = n_features, pot =pot):\n",
    "    \n",
    "    extra_ts = pot - 1\n",
    "    \n",
    "    X = X.iloc[:,:(n_features*(roll+extra_ts))]\n",
    "    y = y.iloc[:,:roll]\n",
    "\n",
    "    y_full = y.dropna()\n",
    "\n",
    "    mask = X.index.isin(y_full.index.tolist())\n",
    "\n",
    "    X_full = X[mask]\n",
    "\n",
    "    y_nan = y[~mask]\n",
    "    X_nan = X[~mask]\n",
    "    \n",
    "    print('NaN')\n",
    "    X_nan, y_nan = reshape_data(X_nan, y_nan, roll)\n",
    "    print('Full')\n",
    "    X_full, y_full = reshape_data(X_full, y_full, roll)\n",
    "    \n",
    "    return X_full, X_nan, y_full, y_nan, mask\n",
    "\n",
    "def provide_all_data(X,y,roll, pot = pot):\n",
    " \n",
    "    extra_ts = pot - 1\n",
    "    \n",
    "    X = X.iloc[:,:n_features*(roll+extra_ts)]\n",
    "    y = y.iloc[:,:roll]\n",
    "\n",
    "    X_all, y_all = reshape_data(X, y, roll)\n",
    "    \n",
    "    return X_all, y_all\n",
    "    \n",
    "def prepare_for_mask(X, y, mask_value = -99):\n",
    "    \"\"\"Improved and working\"\"\"\n",
    "    for i in range(y.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            if np.isnan(y[i][j][0]) or (y[i][j] == mask_value):\n",
    "\n",
    "                X[i][j] = mask_value\n",
    "                y[i][j] = mask_value\n",
    "                \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if (mask_value in X[i][j]) or np.isnan(X[i][j]).any():\n",
    "                \n",
    "                X[i][j] = mask_value\n",
    "                y[i][j] = mask_value\n",
    "            \n",
    "    return X,y\n",
    "\n",
    "def round_off_EDSS(number):\n",
    "    \"\"\"Round a number to the closest half integer.\n",
    "    >>> round_of_rating(1.3)\n",
    "    1.5\n",
    "    >>> round_of_rating(2.6)\n",
    "    2.5\n",
    "    >>> round_of_rating(3.0)\n",
    "    3.0\n",
    "    >>> round_of_rating(4.1)\n",
    "    4.0\"\"\"\n",
    "    return np.round(number * 2) / 2\n",
    "   \n",
    "def def_train_name(f_ix):\n",
    "    \n",
    "    X_train_name = \"data_folds/X_train_f\" + str(f_ix + 1) + \".csv\"\n",
    "    y_train_name = \"data_folds/y_train_f\" + str(f_ix + 1) + \".csv\"\n",
    "    X_test_name = \"data_folds/X_test_f\" + str(f_ix + 1) + \".csv\"\n",
    "    y_test_name = \"data_folds/y_test_f\" + str(f_ix + 1) + \".csv\"\n",
    "    \n",
    "    return X_train_name, X_test_name, y_train_name, y_test_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     9,
     14
    ]
   },
   "outputs": [],
   "source": [
    "weight_file_path = \"weights/my_model_weights_1y6m.h5\"\n",
    "final_file_path = \"final_weights/my_model_weights_1y6m.h5\"\n",
    "best_file_path = \"best/best_weights_1y6m.hdf5\"\n",
    "\n",
    "# reduce learning rate on plateau\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                               patience=5, min_lr=0.001)\n",
    "\n",
    "# stop training if there isn't a significant improvement in the course of 5 epochs\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, \n",
    "                              patience=5, verbose=0, mode='auto', \n",
    "                              baseline=None, restore_best_weights=True)\n",
    "\n",
    "# model check point\n",
    "model_checkpoint = ModelCheckpoint(best_file_path, monitor='val_loss', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True, \n",
    "                                   mode='min')\n",
    "\n",
    "callbacks_list = [reduce_lr, early_stopping]\n",
    "callbacks_list_final = [reduce_lr, early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rnn_model(n_time_steps, n_inputs):\n",
    "    \n",
    "    m = M.Sequential()\n",
    "    m.add(L.Masking(mask_value=-99, input_shape=(n_time_steps, n_inputs)))\n",
    "    m.add(L.GRU(128, return_sequences=True))\n",
    "    m.add(L.Dropout(0.2))\n",
    "    m.add(L.Dense(1, activation='relu'))\n",
    "    m.compile(optimizer = 'adam', loss = 'mean_absolute_error')\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 \n",
    "\n",
    "- Unroll RNN one time step at a time\n",
    "- Transfer weights between rolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     45,
     67,
     74
    ]
   },
   "outputs": [],
   "source": [
    "def generate_initial_predictions(n_time_steps, X_train, y_train, X_test, y_test, n_features=n_features, pot = pot):\n",
    "    \n",
    "    n_inputs = n_features*pot\n",
    "    \n",
    "    threshold = len(y_train.columns)\n",
    "    print(\"TRAIN\")\n",
    "    X_full_train, X_nan_train, y_full_train, y_nan_train, mask_train = provide_data(X_train, y_train, n_time_steps)\n",
    "    print(\"TEST\")\n",
    "    X_full_test, X_nan_test, y_full_test, y_nan_test, mask_test = provide_data(X_test, y_test, n_time_steps)\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    m = rnn_model(n_time_steps, n_inputs)\n",
    "\n",
    "    if n_time_steps > 1:\n",
    "        \n",
    "        # load weights from previous model to establish continuity \n",
    "        m.load_weights(weight_file_path)\n",
    "    \n",
    "    history = m.fit(X_full_train, y_full_train, \n",
    "                    validation_split=0.2,\n",
    "                    batch_size = X_full_train.shape[0], # batch size == size of the training data \n",
    "                    epochs=1, \n",
    "                    shuffle=True,\n",
    "                    callbacks = callbacks_list)\n",
    "        \n",
    "    m.save_weights(weight_file_path) # save weights \n",
    "    \n",
    "    # Predict for unknown values of training and testing data\n",
    "    # Using train data with known targets, I train a model to predict for missing values\n",
    "    # In both training and testing data \n",
    "    # But for the testing data, I only impute the X feature space \n",
    "    \n",
    "    m_pred_train = round_off_EDSS(\n",
    "        pd.Series(np.array([x[0] for x in m.predict(X_nan_train)]).flatten(),\n",
    "                             index = y_train[~mask_train].index))\n",
    "    \n",
    "    # redundant but also helps me known which values to update for test data feature space\n",
    "    m_pred_test = round_off_EDSS(pd.Series(np.array([x[0] for x in m.predict(X_nan_test)]).flatten(),\n",
    "                           index = y_test[~mask_test].index))\n",
    "    \n",
    "    # In pandas the best way to update a dataframe or series\n",
    "    # is with another dataframe or series with the desired index\n",
    "    y_train.iloc[:, n_time_steps-1].update(m_pred_train)\n",
    "    \n",
    "    if (n_time_steps < threshold): # when y contains an EDSS column that X does not \n",
    "        \n",
    "        X_train.iloc[:,(n_features*(n_time_steps+pot))-1].update(m_pred_train)\n",
    "        # redundant but also helps me known which values to update for test data feature space\n",
    "        X_test.iloc[:,(n_features*(n_time_steps+pot))-1].update(m_pred_test)\n",
    "        \n",
    "    if (n_time_steps == threshold):\n",
    "        m.save_weights(final_file_path)\n",
    "        \n",
    "    X_train_all, y_train_all = provide_all_data(X_train, y_test, n_time_steps)\n",
    "    X_test_all, y_test_all = provide_all_data(X_test, y_test, n_time_steps)\n",
    "    \n",
    "    masked_X_train_all, masked_y_train_all = prepare_for_mask(X_train_all, y_train_all)\n",
    "    masked_X_test_all, masked_y_test_all = prepare_for_mask(X_test_all, y_test_all)\n",
    "    \n",
    "    # predict for all values \n",
    "    # use the masking list to known which of these should be used as imputation values\n",
    "    all_train_pred = round_off_EDSS(m.predict(masked_X_train_all))\n",
    "    all_test_pred = round_off_EDSS(m.predict(masked_X_test_all))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, all_train_pred, all_test_pred, mask_train, mask_test \n",
    "\n",
    "def update_previous_predictions(n_time_steps, all_train_pred, all_test_pred):\n",
    "    \n",
    "    m_pred_train = np.array(all_train_pred[:,n_time_steps-1,:]).flatten()\n",
    "    m_pred_test = np.array(all_test_pred[:,n_time_steps-1,:]).flatten()\n",
    "    \n",
    "    return m_pred_train, m_pred_test\n",
    "\n",
    "def impute_previous_predictions(n_time_steps, X_train, X_test,y_train,\n",
    "                                m_pred_train, m_pred_test, mask_list, \n",
    "                                n_features = n_features, pot = pot):\n",
    "    \n",
    "    n_inputs = n_features*pot\n",
    "    \n",
    "    mask_train = mask_list[n_time_steps-1][0]\n",
    "    mask_test = mask_list[n_time_steps-1][1]\n",
    "    \n",
    "    m_pred_train = pd.Series(m_pred_train[~mask_train], index = y_train[~mask_train].index)\n",
    "    m_pred_test = pd.Series(m_pred_test[~mask_test], index = y_test[~mask_test].index)\n",
    "    \n",
    "    y_train.iloc[:, n_time_steps-1].update(m_pred_train)\n",
    "    \n",
    "    if n_time_steps < len(y_train.columns): # 36\n",
    "        \n",
    "        X_train.iloc[:,(n_features*(n_time_steps+pot))-1].update(m_pred_train)\n",
    "        X_test.iloc[:,(n_features*(n_time_steps+pot))-1].update(m_pred_test)\n",
    "    \n",
    "    return X_train, X_test, y_train # y test does not get imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "- Iterate predictive imputation for all time steps \n",
    "- Update all imputated values for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "def exhaustive_imputation(X_train, y_train, X_test, y_test, n_features=n_features, pot = pot):\n",
    "    \n",
    "    \"\"\"Running the model after we've impute for the whole feature space\"\"\"\n",
    "    \n",
    "    n_inputs = n_features*pot\n",
    "    n_time_steps = len(y_train.columns)\n",
    "    \n",
    "    X_train_all, y_train_all = provide_all_data(X_train, y_train, n_time_steps)\n",
    "    X_test_all, y_test_all = provide_all_data(X_test, y_test, n_time_steps)\n",
    "    \n",
    "    masked_X_test_all, masked_y_test_all = prepare_for_mask(X_test_all, y_test_all)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    m = rnn_model(n_time_steps, n_inputs)\n",
    "    \n",
    "    # load weights from previous model to establish continuity \n",
    "    m.load_weights(final_file_path)\n",
    "    \n",
    "    history = m.fit(X_train_all, y_train_all,\n",
    "                    validation_split = 0.2, \n",
    "                    batch_size = X_train_all.shape[0], # of training instances == batch_size \n",
    "                    epochs=1,\n",
    "                    shuffle=True,\n",
    "                   callbacks = callbacks_list_final) # should save best weights \n",
    "    \n",
    "    m.save_weights(final_file_path)\n",
    "    \n",
    "    all_train_pred = round_off_EDSS(m.predict(X_train_all))\n",
    "    all_test_pred = round_off_EDSS(m.predict(masked_X_test_all))\n",
    "    \n",
    "    val_loss = history.__dict__['history']['val_loss']\n",
    "    \n",
    "    return all_train_pred, all_test_pred, val_loss, m\n",
    "\n",
    "def iterate_exhaustive_imputation(n_epochs, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    We divide our RNN into different 1 epoch models \n",
    "    in order to iteratively update the imputations\n",
    "    \"\"\"\n",
    "    val_loss_list = []\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        print('EPOCH: ', str(i))\n",
    "        all_train_pred, all_test_pred, val_loss, model = exhaustive_imputation(X_train, y_train, \n",
    "                                                                               X_test, y_test)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "    \n",
    "        for i in range(1, all_train_pred.shape[1]): # -1 \n",
    "\n",
    "                    m_pred_train, m_pred_test = update_previous_predictions(i, all_train_pred, \n",
    "                                                                            all_test_pred)    \n",
    "                    \n",
    "                    X_train, X_test, y_train = impute_previous_predictions(i, X_train, X_test,\n",
    "                                                                           y_train, m_pred_train, \n",
    "                                                                           m_pred_test, mask_list)\n",
    "    \n",
    "    return np.min(val_loss_list), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "- Generate final imputation and return imputed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     26
    ]
   },
   "outputs": [],
   "source": [
    "def best_weights_imputation(X_train, y_train, X_test, y_test, n_features=n_features, pot = pot):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns EDSS imputations using best model weights\n",
    "    Running the model after we've impute for the whole feature space\"\"\"\n",
    "    n_inputs = n_features*pot\n",
    "    n_time_steps = len(y_train.columns)\n",
    "    \n",
    "    X_train_all, y_train_all = provide_all_data(X_train, y_train, n_time_steps)\n",
    "    X_test_all, y_test_all = provide_all_data(X_test, y_test, n_time_steps)\n",
    "    \n",
    "    # we do want to mask the instances of the testing data where there are missing targets\n",
    "    # so that it does not affect our val_loss\n",
    "    masked_X_test_all, masked_y_test_all = prepare_for_mask(X_test_all, y_test_all)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    m = rnn_model(n_time_steps, n_inputs)\n",
    "    # load weights from best weights model to generate final imputation values \n",
    "    m.load_weights(best_file_path)\n",
    "    \n",
    "    all_train_pred = round_off_EDSS(m.predict(X_train_all))\n",
    "    all_test_pred = round_off_EDSS(m.predict(masked_X_test_all))\n",
    "    \n",
    "    return all_train_pred, all_test_pred\n",
    "\n",
    "def return_imputed_data(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Part 2\n",
    "    Returns final data sets imputed with best model weights\n",
    "    [Feature EDSS (train and test); target EDSS (train)]\n",
    "    \"\"\"\n",
    "    \n",
    "    val_loss_list = []\n",
    "    \n",
    "    all_train_pred, all_test_pred = best_weights_imputation(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    for i in range(1, all_train_pred.shape[1]): # -1 \n",
    "\n",
    "                m_pred_train, m_pred_test = update_previous_predictions(i, all_train_pred, all_test_pred)    \n",
    "                    \n",
    "                X_train, X_test, y_train = impute_previous_predictions(i, X_train, X_test,y_train, \n",
    "                                                                       m_pred_train, m_pred_test, \n",
    "                                                                       mask_list)\n",
    "    \n",
    "    n_time_steps = len(y_test.columns)\n",
    "    \n",
    "    X_test_all, y_test_all = provide_all_data(X_test, y_test, n_time_steps)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../../../data/pre_imputation_data/X_1.5_years|6_months.csv', index_col = 0)\n",
    "y = pd.read_csv('../../../data/pre_imputation_data/y_1.5_years|6_months.csv', index_col = 0)\n",
    "\n",
    "#n_time_steps = len(y.columns)\n",
    "n_features = X.columns.tolist().index(\"EDSS_0\")+1\n",
    "pot = 3\n",
    "n_inputs = n_features * pot\n",
    "n_units = 128\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"The input length of the training data will be\", pot, \"time slices, separated by 6 month intervals\")\n",
    "print(n_features, \"featues comprise one time slice\")\n",
    "\n",
    "# Fill in first X\n",
    "first_X_ffill = X[['EDSS_0','EDSS_6','EDSS_12']].fillna(method = 'ffill',  axis = 1)\n",
    "for col in first_X_ffill.columns:\n",
    "    if col in X.columns:\n",
    "         X[col] = first_X_ffill[col]\n",
    "        \n",
    "# Drop all missing values from EDSS_0 which is our first X feature predictor\n",
    "mask = X.index.isin(X[['EDSS_0','EDSS_6', 'EDSS_12']].dropna().index.tolist())\n",
    "\n",
    "X_og, y_og = X[mask], y[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide Data into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "for f_ix in range(5): # already did f_ix = 0, data fold 1\n",
    "\n",
    "    train_ix = []\n",
    "    test_ix = []\n",
    "    for train, test in skf.split(X_og.index):\n",
    "        train_ix.append(train)\n",
    "        test_ix.append(test)\n",
    "\n",
    "    X_train_og, y_train_og = X_og.iloc[train_ix[f_ix], :].copy(), y_og.iloc[train_ix[f_ix], :].copy()\n",
    "    X_test_og, y_test_og = X_og.iloc[test_ix[f_ix], :].copy(), y_og.iloc[test_ix[f_ix], :].copy()\n",
    "\n",
    "    # Run algorithm\n",
    "\n",
    "    mask_list = []\n",
    "\n",
    "    X_train, X_test, y_train, y_test, all_train_pred, all_test_pred, mask_train, mask_test = generate_initial_predictions(1, X_train_og, \n",
    "                                                                                        y_train_og, \n",
    "                                                                                        X_test_og, \n",
    "                                                                                        y_test_og)\n",
    "\n",
    "    mask_list.append((mask_train, mask_test)) # these tell me what values are missing in each column \n",
    "\n",
    "    # Sanity Check\n",
    "\n",
    "    print(\"There are missing values for EDSS in the X feature space:\", pd.isna(X_train['EDSS_18']).any())\n",
    "    print(\"There are missing values for EDSS in the y target space:\", pd.isna(y_train['EDSS_18']).any())\n",
    "\n",
    "    for r in range(2, len(y_train.columns)+1):\n",
    "\n",
    "        X_train, X_test, y_train, y_test, all_train_pred, all_test_pred, mask_train, mask_test = generate_initial_predictions(r, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        mask_list.append((mask_train, mask_test))\n",
    "\n",
    "        if (all_train_pred.shape[1] > 1):\n",
    "\n",
    "            # update all previous predictions\n",
    "            for col in range(1,all_train_pred.shape[1]):\n",
    "                # for each column of the training and testing EDSS\n",
    "                m_pred_train, m_pred_test = update_previous_predictions(col, all_train_pred, all_test_pred)    \n",
    "                X_train, X_test, y_train = impute_previous_predictions(col, X_train, X_test, y_train,m_pred_train, m_pred_test, mask_list)\n",
    "\n",
    "\n",
    "    # X_train should go up to 210\n",
    "\n",
    "    save_temp = (X_train, X_test, y_train, y_test, all_train_pred, all_test_pred, mask_train, mask_test)\n",
    "    save_list_temp = mask_list.copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test, all_train_pred, all_test_pred, mask_train, mask_test = save_temp \n",
    "    mask_list = save_list_temp\n",
    "\n",
    "    # Run final model for training with whole feature space and training targets\n",
    "\n",
    "    best, model = iterate_exhaustive_imputation(100, X_train, y_train, X_test, y_test) # use original y test\n",
    "\n",
    "    best\n",
    "\n",
    "    # Generate optimal imputation \n",
    "\n",
    "    X_train, X_test, y_train, y_test =  return_imputed_data(X_train, X_test, y_train, y_test) # use original y test\n",
    "\n",
    "    # Use original unimputed test data and save data\n",
    "\n",
    "    X_test_no_imputation, y_test_no_imputation = X_og.iloc[test_ix[f_ix], :].copy(), y_og.iloc[test_ix[f_ix], :].copy()\n",
    "\n",
    "    # save data folds\n",
    "    a,b,c,d = def_train_name(f_ix)\n",
    "\n",
    "    X_train.to_csv(a)\n",
    "    X_test_no_imputation.to_csv(b)\n",
    "    y_train.to_csv(c)\n",
    "    y_test_no_imputation.to_csv(d)\n",
    "    \n",
    "    X_test.to_csv(\"unused_data_folds/X_test_\"+str(f_ix+1)+\".csv\")\n",
    "    y_test.to_csv(\"unused_data_folds/y_test_\"+str(f_ix+1)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
